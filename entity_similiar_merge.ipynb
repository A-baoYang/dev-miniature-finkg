{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top similiar entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將事件分祠但保留實體\n",
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "# 將上述結果丟入 Word2Vec 建立以語料為範圍之 embedding space 用來找出最相似的詞彙\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import json\n",
    "import numpy as np\n",
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abao.yang@cathayholdings.com.tw/.conda/envs/afa_chatbot/lib/python3.8/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:901: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "/home/abao.yang@cathayholdings.com.tw/.conda/envs/afa_chatbot/lib/python3.8/site-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n",
      "2022-06-16 00:41:15.721178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:15.884830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:15.885499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:17.396461: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-16 00:41:17.397980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:17.398830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:17.399475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:19.035868: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:19.036604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:19.037366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-16 00:41:19.038004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13805 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "ckip_ws = WS(\"ckiptagger/data\", disable_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '計程車每輛補助5千元防疫加清消費用 7月一次性撥付 | 金融脈動 | 金融 | 經濟日報',\n",
       " 'url': 'https://money.udn.com/money/story/5613/6381896?from=edn_newestlist_rank',\n",
       " 'meta_keywords': '交通部疫情觀光',\n",
       " 'article': '計程車每輛補助5千元防疫加清消費用 7月一次性撥付 | 金融脈動 | 金融 | 經濟日報為協助司機駕駛因應疫情高原期影響，交通部表示，行政院已同意延長從6月至年底的防疫物資費用補貼，並已規畫延續每日防疫補助以及新增一次性清消防護費用，提高每輛計程車補貼金額共5000元，交通部也已請公路總局盡速完成準備作業，於7月進行撥付。交通部表示，為協助計程車營運防疫所需，自110年5月起，每天補貼計程車防疫物資費用15元，每個月補貼26天，原本至今年5月底止，在行政院支持下將延長至今年年底。考量計程車每日均載運不特定乘客且空間小與乘客近，因此除延續每日防疫物資補貼6至12月底的2730元，再新增一次性因應現況疫情加強清消及防護費用2270元，每輛計程車共計5000元，將於7月以一次性撥款方式匯到計程車車主帳戶，經費由交通部防疫預算支出。 針對遊覽車和租賃小客車短租車輛，交通部也規畫階段性的汽燃費減徵支持振興措施。在遊覽車部分，以減免徵111年第3季汽燃費為實施措施，出車率維持7成以上者減半徵收，未達7成者免徵，平均每輛車可節省約4000元至1萬元不等。小客車租賃業部分，因長租車均有固定承租客户，故規畫針對短租車提供111年第3季汽燃費減半徵收方案，平均每輛車可節省約1200元至1800元。交通部說，因應國內疫情及依據行政院現階段以防疫及振興為主的政策，針對目前仍受影響的觀光、公路客運、計程車、遊覽車及小客車租賃等產業，在行政院支持下，尋求各種可運用的資源，已分別因應各產業規畫不同的方案措施，協助業者因應疫情的影響，並將持續與各產業同步做好的振興準備。交通部擬發給計程車業每人一次性5000元防疫物資補貼。記者余承翰／攝影',\n",
       " 'publish_time': '2022-06-12 09:54:27',\n",
       " 'media': 'MoneyUDN',\n",
       " 'event_triplets': [{'event': ['公路總局', '補助', '防疫加清消費用'],\n",
       "   'entity_extract': {'ORG': ['公路總局'], 'OTH': ['防疫']}},\n",
       "  {'event': ['司機', '因應', '疫情高原期影響'], 'entity_extract': {'OTH': ['疫情']}},\n",
       "  {'event': ['交通部',\n",
       "    '表示',\n",
       "    '行政院已同意延長從6月至年底防疫物資費用補貼並已規畫延續每日防疫補助以及新增一次性清消防護費用提高每輛計程車補貼金額共5000元'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'MONEY': ['5000元'],\n",
       "    'DATE': ['6月至年底'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫', '防疫物資費用補貼', '防疫補助', '防護費用', '計程車補貼金額'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['行政院', '同意', '延長從6月至年底防疫物資費用補貼'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'DATE': ['6月至年底'],\n",
       "    'OTH': ['防疫', '防疫物資費用補貼']}},\n",
       "  {'event': ['交通部', '請', '公路總局'], 'entity_extract': {'ORG': ['公路總局']}},\n",
       "  {'event': ['公路總局', '完成', '準備作業'],\n",
       "   'entity_extract': {'ORG': ['公路總局'], 'OTH': ['作業']}},\n",
       "  {'event': ['公路總局', '進行', '撥付'], 'entity_extract': {'ORG': ['公路總局']}},\n",
       "  {'event': ['交通部',\n",
       "    '表示',\n",
       "    '為協助計程車營運防疫需自110年5月起每天補貼計程車防疫物資費用15元每個月補貼26天原本至今年5月底止在行政院支持下將延長至今年年底'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'MONEY': ['15元'],\n",
       "    'DATE': ['26天', '110年5月', '今年年底', '今年5月底止'],\n",
       "    'OTH': ['防疫', '計程車營運防疫', '補貼計程車防疫物資費用'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['計程車', '載運', '不特定乘客'], 'entity_extract': {'PRODUCT': ['計程車']}},\n",
       "  {'event': ['由交通部防疫預算', '支出', '經費'],\n",
       "   'entity_extract': {'ORG': ['交通部'], 'OTH': ['防疫']}},\n",
       "  {'event': ['交通部', '規畫', '階段性汽燃費減徵支持振興措施'],\n",
       "   'entity_extract': {'ORG': ['交通部']}},\n",
       "  {'event': ['者', '達', '7成'], 'entity_extract': {'PERCENT': ['7成']}},\n",
       "  {'event': ['每輛車', '節省', '約4000元至1萬元'],\n",
       "   'entity_extract': {'MONEY': ['4000元']}},\n",
       "  {'event': ['長租車', '有', '固定承租客户'], 'entity_extract': {}},\n",
       "  {'event': ['小客車租賃業部分', '規畫', '針對短租車提供111年第3季汽燃費減半徵收方案'],\n",
       "   'entity_extract': {'DATE': ['111年第3季', '第3季', '11年']}},\n",
       "  {'event': ['每輛車', '節省', '約1200元至1800元'],\n",
       "   'entity_extract': {'MONEY': ['1200元', '1200元至1800元']}},\n",
       "  {'event': ['交通部',\n",
       "    '說',\n",
       "    '因應國內疫情及依據行政院現階段以防疫及振興為主政策針對目前仍受影響觀光公路客運計程車遊覽車及小客車租賃產業在行政院支持下尋求各種可運用資源已分別因應各產業規畫不同方案措施協助業者因應疫情影響並將持續與各產業同步做好振興準備'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'OTH': ['防疫', '疫情'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['產業', '為主', '以防疫及振興'], 'entity_extract': {'OTH': ['防疫']}},\n",
       "  {'event': ['產業', '受', '影響'], 'entity_extract': {}},\n",
       "  {'event': ['業者', '因應', '疫情影響'], 'entity_extract': {'OTH': ['疫情']}},\n",
       "  {'event': ['交通部', '擬', '發給計程車業每人一次性5000元防疫物資補貼'],\n",
       "   'entity_extract': {'ORG': ['交通部'],\n",
       "    'MONEY': ['5000元'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['應疫情高原期影響，交通部表示，行政院已同意延長從6月至年底的防疫物資費用補貼，並已規畫延續每日防疫補助以及新增一次性清消防護費用，提高每輛計程車補貼金額共5000元，交通部也已請公路總局盡速完成準備作業',\n",
       "    '因',\n",
       "    '於7月進行撥付。'],\n",
       "   'entity_extract': {'MONEY': ['5000元'],\n",
       "    'ORG': ['公路總局', '行政院', '交通部'],\n",
       "    'DATE': ['7月'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫', '疫情', '防疫物資費用補貼', '防疫補助', '防護費用', '計程車補貼金額', '作業'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['考量計程車每日均載運不特定乘客且空間小與乘客近',\n",
       "    '因此',\n",
       "    '除延續每日防疫物資補貼6至12月底的2730元，再新增一次性因應現況疫情加強清消及防護費用2270元，每輛計程車共計5000元，將於7月以一次性撥款方式匯到計程車車主帳戶，經費由交通部防疫預算支出。'],\n",
       "   'entity_extract': {'PRODUCT': ['計程車'],\n",
       "    'MONEY': ['2270元', '5000元', '2730元'],\n",
       "    'ORG': ['交通部'],\n",
       "    'DATE': ['7月', '6至12月底'],\n",
       "    'TIME': ['6至12月底'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫', '疫情', '防護費用']}},\n",
       "  {'event': ['小客車租賃業部分，因長租車均有固定承租客户',\n",
       "    '故',\n",
       "    '規畫針對短租車提供111年第3季汽燃費減半徵收方案，平均每輛車可節省約1200元至1800元。'],\n",
       "   'entity_extract': {'MONEY': ['1200元', '1200元至1800元'],\n",
       "    'DATE': ['111年第3季', '第3季', '11年']}},\n",
       "  {'event': ['應國內疫情及依據行政院現階段以防疫及振興為主的政策，針對目前仍受影響的觀光、公路客運、計程車、遊覽車及小客車租賃等產業，在行政院支持下，尋求各種可運用的資源，已分別因應各產業規畫不同的方案措施，協助業者因應疫情的影響',\n",
       "    '因',\n",
       "    '並將持續與各產業同步做好的振興準備。'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'OTH': ['防疫', '疫情'],\n",
       "    'PRODUCT': ['計程車']}}],\n",
       " 'entity_extract': {'MONEY': ['2270元',\n",
       "   '5000元',\n",
       "   '5千元',\n",
       "   '4000元',\n",
       "   '1200元',\n",
       "   '15元',\n",
       "   '2730元',\n",
       "   '1200元至1800元'],\n",
       "  'ORG': ['公路總局', '行政院', '交通部'],\n",
       "  'DATE': ['111年第3季',\n",
       "   '7月',\n",
       "   '6月至年底',\n",
       "   '第3季',\n",
       "   '26天',\n",
       "   '110年5月',\n",
       "   '今年年底',\n",
       "   '今年5月底止',\n",
       "   '6至12月底',\n",
       "   '11年'],\n",
       "  'PERCENT': ['7成'],\n",
       "  'WORK_OF_ART': ['經濟日報'],\n",
       "  'PERSON': ['余承翰'],\n",
       "  'TIME': ['6至12月底'],\n",
       "  'CARDINAL': ['一次性'],\n",
       "  'ART': [],\n",
       "  'OTH': ['防疫',\n",
       "   '金融脈動',\n",
       "   '金融',\n",
       "   '疫情',\n",
       "   '防疫物資費用補貼',\n",
       "   '防疫補助',\n",
       "   '防護費用',\n",
       "   '計程車補貼金額',\n",
       "   '作業',\n",
       "   '計程車營運防疫',\n",
       "   '補貼計程車防疫物資費用'],\n",
       "  'EVENT': [],\n",
       "  'PRODUCT': ['計程車']}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_filepath = os.path.join(\n",
    "    \"model_data\", \"output\", \"entity_extraction-output-20220614.json\"\n",
    ")\n",
    "with open(input_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [05:07<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['公路總局', '補助', '防疫', '加', '清', '消費用'],\n",
       " ['司機', '因應', '疫情', '高原期', '影響'],\n",
       " ['交通部',\n",
       "  '表示',\n",
       "  '行政院',\n",
       "  '已',\n",
       "  '同意',\n",
       "  '延長',\n",
       "  '從',\n",
       "  '6月至年底',\n",
       "  '防疫物資費用補貼',\n",
       "  '並',\n",
       "  '已',\n",
       "  '規畫',\n",
       "  '延續',\n",
       "  '每日',\n",
       "  '防疫補助',\n",
       "  '以及',\n",
       "  '新增',\n",
       "  '一次性',\n",
       "  '清消',\n",
       "  '防護費用',\n",
       "  '提高',\n",
       "  '每',\n",
       "  '輛',\n",
       "  '計程車補貼金額',\n",
       "  '共',\n",
       "  '5000元'],\n",
       " ['行政院', '同意', '延長', '從', '6月至年底', '防疫物資費用補貼'],\n",
       " ['交通部', '請', '公路總局'],\n",
       " ['公路總局', '完成', '準備', '作業'],\n",
       " ['公路總局', '進行', '撥付'],\n",
       " ['交通部',\n",
       "  '表示',\n",
       "  '為',\n",
       "  '協助',\n",
       "  '計程車營運防疫',\n",
       "  '需',\n",
       "  '自',\n",
       "  '110年5月',\n",
       "  '起',\n",
       "  '每',\n",
       "  '天',\n",
       "  '補貼計程車防疫物資費用',\n",
       "  '15元',\n",
       "  '每',\n",
       "  '個',\n",
       "  '月',\n",
       "  '補貼',\n",
       "  '26天',\n",
       "  '原本',\n",
       "  '至',\n",
       "  '今年5月底止',\n",
       "  '在',\n",
       "  '行政院',\n",
       "  '支持',\n",
       "  '下',\n",
       "  '將',\n",
       "  '延長',\n",
       "  '至',\n",
       "  '今年年底'],\n",
       " ['計程車', '載運', '不', '特定', '乘客'],\n",
       " ['由', '交通部', '防疫', '預算', '支出', '經費']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將事件實體根據順序拼出句子當作語料庫\n",
    "ws_sents = []\n",
    "for article in tqdm(data):\n",
    "    entities = []\n",
    "    for type, ent_list in article[\"entity_extract\"].items():\n",
    "        entities += ent_list\n",
    "    entities = np.unique(entities).tolist()\n",
    "    word_weight = {chr: 2 for chr in entities}\n",
    "    for event_set in article[\"event_triplets\"]:\n",
    "        event = event_set[\"event\"]\n",
    "        ws_sent = []\n",
    "        for i in range(len(event)):\n",
    "            if i == 1:\n",
    "                _ws = [event[i]]\n",
    "            else:\n",
    "                _ws = ckip_ws(\n",
    "                    [event[i]], \n",
    "                    coerce_dictionary=construct_dictionary(word_weight)\n",
    "                )[0]\n",
    "            ws_sent += _ws\n",
    "        ws_sents.append(ws_sent)\n",
    "\n",
    "print(len(ws_sents))\n",
    "ws_sents[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.63s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.64s/it]\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.64s/it]\n",
      "100%|██████████| 1/1 [00:38<00:00, 38.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec train settings\n",
    "seed = 888\n",
    "sgs = [1] #[0, 1]\n",
    "window_sizes = [10] #[1, 2, 3, 4, 5]\n",
    "vector_sizes = [500] #[50, 100, 300, 500, 1000, 2000]\n",
    "min_count = 2\n",
    "workers = 8\n",
    "epochs = [100] #[30, 50, 100]\n",
    "\n",
    "\n",
    "# with open(f'external_output/word2vec_models/logging-{kw_col}.txt', 'w', encoding='utf-8') as f:\n",
    "#     f.write(f'parameters: \\nseed: {seed}\\nwindow_size: {window_size}\\nmin_count: {min_count}\\nworkers: {workers}\\n\\n')\n",
    "\n",
    "for epoch in tqdm(epochs):\n",
    "    for sg in tqdm(sgs):\n",
    "        for window_size in tqdm(window_sizes):\n",
    "            for vector_size in tqdm(vector_sizes):\n",
    "\n",
    "                model = word2vec.Word2Vec(\n",
    "                    ws_sents,  # 語料\n",
    "                    min_count=min_count,  # 詞彙的最小頻率，小於則從語料中排除\n",
    "                    vector_size=vector_size,  # 詞向量維度\n",
    "                    workers=workers,  # 使用多少 worker threads 訓練模型\n",
    "                    epochs=epoch,  # iterations on this corpus\n",
    "                    window=window_size,  # 多遠之內的詞要納入相似判斷範圍\n",
    "                    sg=sg,  # 0=CBOW, 1=skip-gram\n",
    "                    seed=seed,  # 向量初始化所需的隨機亂數生成\n",
    "                    shrink_windows=True\n",
    "                )\n",
    "                \n",
    "                # with open(f'external_output/word2vec_models/logging-{kw_col}.txt', 'a', encoding='utf-8') as f:\n",
    "                #     f.write(f'===== epoch: {epoch} & sg: {sg} & vector_size: {vector_size} ===== \\n')\n",
    "                #     for test_word in ['信用卡','Line']:\n",
    "                #         f.write(f'test word: {test_word}\\n')\n",
    "                #         res = model.wv.most_similar(test_word)\n",
    "                #         for item in res:\n",
    "                #             f.write(str(item))\n",
    "                #             f.write('\\n')\n",
    "                #         f.write('\\n')\n",
    "                #     f.write('\\n')\n",
    "\n",
    "# model.save(f'external_output/word2vec/models/afa-{kw_col}-word2vec-epoch_{epoch}-sg_{sg}-vector_size_{vector_size}-window_size_{window_size}.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5000元', 0.6660891175270081),\n",
       " ('一次性', 0.6519504189491272),\n",
       " ('車主', 0.641745924949646),\n",
       " ('計程車營運防疫', 0.6352728009223938),\n",
       " ('階段性', 0.6295223832130432),\n",
       " ('110年5月', 0.6273397207260132),\n",
       " ('減徵', 0.6254816651344299),\n",
       " ('計程車補貼金額', 0.6141300201416016),\n",
       " ('振興措施', 0.6136227250099182),\n",
       " ('6月至年底', 0.6130661368370056)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"交通部\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Embeddings \n",
    "- 哈爾濱工業大學 中文全詞遮罩 chinese-roberta-wwm-ext-large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 測試採用幾層 Embeddings 及不同 pooling strategy 對相似度的差異"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "from KGBuilder.config import *\n",
    "from KGBuilder.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 有部分可以針對把相似詞合併，但有一大群模稜兩可的實體被放到一起，再針對最大群進行分群\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext-large\")\n",
    "model = BertModel.from_pretrained(\n",
    "    \"hfl/chinese-roberta-wwm-ext-large\",\n",
    "    output_hidden_states = True\n",
    ")\n",
    "device = torch.device(\"cuda\")\n",
    "model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '計程車每輛補助5千元防疫加清消費用 7月一次性撥付 | 金融脈動 | 金融 | 經濟日報',\n",
       " 'url': 'https://money.udn.com/money/story/5613/6381896?from=edn_newestlist_rank',\n",
       " 'meta_keywords': '交通部疫情觀光',\n",
       " 'article': '計程車每輛補助5千元防疫加清消費用 7月一次性撥付 | 金融脈動 | 金融 | 經濟日報為協助司機駕駛因應疫情高原期影響，交通部表示，行政院已同意延長從6月至年底的防疫物資費用補貼，並已規畫延續每日防疫補助以及新增一次性清消防護費用，提高每輛計程車補貼金額共5000元，交通部也已請公路總局盡速完成準備作業，於7月進行撥付。交通部表示，為協助計程車營運防疫所需，自110年5月起，每天補貼計程車防疫物資費用15元，每個月補貼26天，原本至今年5月底止，在行政院支持下將延長至今年年底。考量計程車每日均載運不特定乘客且空間小與乘客近，因此除延續每日防疫物資補貼6至12月底的2730元，再新增一次性因應現況疫情加強清消及防護費用2270元，每輛計程車共計5000元，將於7月以一次性撥款方式匯到計程車車主帳戶，經費由交通部防疫預算支出。 針對遊覽車和租賃小客車短租車輛，交通部也規畫階段性的汽燃費減徵支持振興措施。在遊覽車部分，以減免徵111年第3季汽燃費為實施措施，出車率維持7成以上者減半徵收，未達7成者免徵，平均每輛車可節省約4000元至1萬元不等。小客車租賃業部分，因長租車均有固定承租客户，故規畫針對短租車提供111年第3季汽燃費減半徵收方案，平均每輛車可節省約1200元至1800元。交通部說，因應國內疫情及依據行政院現階段以防疫及振興為主的政策，針對目前仍受影響的觀光、公路客運、計程車、遊覽車及小客車租賃等產業，在行政院支持下，尋求各種可運用的資源，已分別因應各產業規畫不同的方案措施，協助業者因應疫情的影響，並將持續與各產業同步做好的振興準備。交通部擬發給計程車業每人一次性5000元防疫物資補貼。記者余承翰／攝影',\n",
       " 'publish_time': '2022-06-12 09:54:27',\n",
       " 'media': 'MoneyUDN',\n",
       " 'event_triplets': [{'event': ['公路總局', '補助', '防疫加清消費用'],\n",
       "   'entity_extract': {'ORG': ['公路總局'], 'OTH': ['防疫']}},\n",
       "  {'event': ['司機', '因應', '疫情高原期影響'], 'entity_extract': {'OTH': ['疫情']}},\n",
       "  {'event': ['交通部',\n",
       "    '表示',\n",
       "    '行政院已同意延長從6月至年底防疫物資費用補貼並已規畫延續每日防疫補助以及新增一次性清消防護費用提高每輛計程車補貼金額共5000元'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'MONEY': ['5000元'],\n",
       "    'DATE': ['6月至年底'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫', '防疫物資費用補貼', '防疫補助', '防護費用', '計程車補貼金額'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['行政院', '同意', '延長從6月至年底防疫物資費用補貼'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'DATE': ['6月至年底'],\n",
       "    'OTH': ['防疫', '防疫物資費用補貼']}},\n",
       "  {'event': ['交通部', '請', '公路總局'], 'entity_extract': {'ORG': ['公路總局']}},\n",
       "  {'event': ['公路總局', '完成', '準備作業'],\n",
       "   'entity_extract': {'ORG': ['公路總局'], 'OTH': ['作業']}},\n",
       "  {'event': ['公路總局', '進行', '撥付'], 'entity_extract': {'ORG': ['公路總局']}},\n",
       "  {'event': ['交通部',\n",
       "    '表示',\n",
       "    '為協助計程車營運防疫需自110年5月起每天補貼計程車防疫物資費用15元每個月補貼26天原本至今年5月底止在行政院支持下將延長至今年年底'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'MONEY': ['15元'],\n",
       "    'DATE': ['26天', '110年5月', '今年年底', '今年5月底止'],\n",
       "    'OTH': ['防疫', '計程車營運防疫', '補貼計程車防疫物資費用'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['計程車', '載運', '不特定乘客'], 'entity_extract': {'PRODUCT': ['計程車']}},\n",
       "  {'event': ['由交通部防疫預算', '支出', '經費'],\n",
       "   'entity_extract': {'ORG': ['交通部'], 'OTH': ['防疫']}},\n",
       "  {'event': ['交通部', '規畫', '階段性汽燃費減徵支持振興措施'],\n",
       "   'entity_extract': {'ORG': ['交通部']}},\n",
       "  {'event': ['者', '達', '7成'], 'entity_extract': {'PERCENT': ['7成']}},\n",
       "  {'event': ['每輛車', '節省', '約4000元至1萬元'],\n",
       "   'entity_extract': {'MONEY': ['4000元']}},\n",
       "  {'event': ['長租車', '有', '固定承租客户'], 'entity_extract': {}},\n",
       "  {'event': ['小客車租賃業部分', '規畫', '針對短租車提供111年第3季汽燃費減半徵收方案'],\n",
       "   'entity_extract': {'DATE': ['111年第3季', '第3季', '11年']}},\n",
       "  {'event': ['每輛車', '節省', '約1200元至1800元'],\n",
       "   'entity_extract': {'MONEY': ['1200元', '1200元至1800元']}},\n",
       "  {'event': ['交通部',\n",
       "    '說',\n",
       "    '因應國內疫情及依據行政院現階段以防疫及振興為主政策針對目前仍受影響觀光公路客運計程車遊覽車及小客車租賃產業在行政院支持下尋求各種可運用資源已分別因應各產業規畫不同方案措施協助業者因應疫情影響並將持續與各產業同步做好振興準備'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'OTH': ['防疫', '疫情'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['產業', '為主', '以防疫及振興'], 'entity_extract': {'OTH': ['防疫']}},\n",
       "  {'event': ['產業', '受', '影響'], 'entity_extract': {}},\n",
       "  {'event': ['業者', '因應', '疫情影響'], 'entity_extract': {'OTH': ['疫情']}},\n",
       "  {'event': ['交通部', '擬', '發給計程車業每人一次性5000元防疫物資補貼'],\n",
       "   'entity_extract': {'ORG': ['交通部'],\n",
       "    'MONEY': ['5000元'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['應疫情高原期影響，交通部表示，行政院已同意延長從6月至年底的防疫物資費用補貼，並已規畫延續每日防疫補助以及新增一次性清消防護費用，提高每輛計程車補貼金額共5000元，交通部也已請公路總局盡速完成準備作業',\n",
       "    '因',\n",
       "    '於7月進行撥付。'],\n",
       "   'entity_extract': {'MONEY': ['5000元'],\n",
       "    'ORG': ['公路總局', '行政院', '交通部'],\n",
       "    'DATE': ['7月'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫', '疫情', '防疫物資費用補貼', '防疫補助', '防護費用', '計程車補貼金額', '作業'],\n",
       "    'PRODUCT': ['計程車']}},\n",
       "  {'event': ['考量計程車每日均載運不特定乘客且空間小與乘客近',\n",
       "    '因此',\n",
       "    '除延續每日防疫物資補貼6至12月底的2730元，再新增一次性因應現況疫情加強清消及防護費用2270元，每輛計程車共計5000元，將於7月以一次性撥款方式匯到計程車車主帳戶，經費由交通部防疫預算支出。'],\n",
       "   'entity_extract': {'PRODUCT': ['計程車'],\n",
       "    'MONEY': ['2270元', '5000元', '2730元'],\n",
       "    'ORG': ['交通部'],\n",
       "    'DATE': ['7月', '6至12月底'],\n",
       "    'TIME': ['6至12月底'],\n",
       "    'CARDINAL': ['一次性'],\n",
       "    'OTH': ['防疫', '疫情', '防護費用']}},\n",
       "  {'event': ['小客車租賃業部分，因長租車均有固定承租客户',\n",
       "    '故',\n",
       "    '規畫針對短租車提供111年第3季汽燃費減半徵收方案，平均每輛車可節省約1200元至1800元。'],\n",
       "   'entity_extract': {'MONEY': ['1200元', '1200元至1800元'],\n",
       "    'DATE': ['111年第3季', '第3季', '11年']}},\n",
       "  {'event': ['應國內疫情及依據行政院現階段以防疫及振興為主的政策，針對目前仍受影響的觀光、公路客運、計程車、遊覽車及小客車租賃等產業，在行政院支持下，尋求各種可運用的資源，已分別因應各產業規畫不同的方案措施，協助業者因應疫情的影響',\n",
       "    '因',\n",
       "    '並將持續與各產業同步做好的振興準備。'],\n",
       "   'entity_extract': {'ORG': ['行政院'],\n",
       "    'OTH': ['防疫', '疫情'],\n",
       "    'PRODUCT': ['計程車']}}],\n",
       " 'entity_extract': {'MONEY': ['2270元',\n",
       "   '5000元',\n",
       "   '5千元',\n",
       "   '4000元',\n",
       "   '1200元',\n",
       "   '15元',\n",
       "   '2730元',\n",
       "   '1200元至1800元'],\n",
       "  'ORG': ['公路總局', '行政院', '交通部'],\n",
       "  'DATE': ['111年第3季',\n",
       "   '7月',\n",
       "   '6月至年底',\n",
       "   '第3季',\n",
       "   '26天',\n",
       "   '110年5月',\n",
       "   '今年年底',\n",
       "   '今年5月底止',\n",
       "   '6至12月底',\n",
       "   '11年'],\n",
       "  'PERCENT': ['7成'],\n",
       "  'WORK_OF_ART': ['經濟日報'],\n",
       "  'PERSON': ['余承翰'],\n",
       "  'TIME': ['6至12月底'],\n",
       "  'CARDINAL': ['一次性'],\n",
       "  'ART': [],\n",
       "  'OTH': ['防疫',\n",
       "   '金融脈動',\n",
       "   '金融',\n",
       "   '疫情',\n",
       "   '防疫物資費用補貼',\n",
       "   '防疫補助',\n",
       "   '防護費用',\n",
       "   '計程車補貼金額',\n",
       "   '作業',\n",
       "   '計程車營運防疫',\n",
       "   '補貼計程車防疫物資費用'],\n",
       "  'EVENT': [],\n",
       "  'PRODUCT': ['計程車']}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_filepath = os.path.join(\n",
    "    \"model_data\", \"output\", \"entity_extraction-output-20220614.json\"\n",
    ")\n",
    "with open(input_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "len(data)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 將實體區間標出後跑詞彙本身的 K-Means 分群\n",
    "1. 取出完整事件句子及實體位置，轉換為 BERT Word Embeddings\n",
    "2. 針對每個實體的 Word Embeddings 進行分群（K-Means 使用的也是 cosine simliarity）\n",
    "   1. 先不去除所有句子統計下來的重複實體 => 因為上下文不同時的意思可能不同，如果將不同句子內的實體 embedding 先 mean 或 sum 處理，不確定會不會缺失原有語意；可能保留句子本身再比對較佳\n",
    "     - 效果不太好，重複實體在不同句子的向量不同，但因為接近優先被聚到一群；其他相似詞的 threshold 變得很難調整\n",
    "   2. 先將重複實體全部平均到一起，最後結果不同在哪句，只看實體本身"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 取出完整事件句子及實體位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [00:00<00:00, 732.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# 紀錄每個句子之實體位置\n",
    "for article in tqdm(data):\n",
    "    for event_set in article[\"event_triplets\"]:\n",
    "        event_verb = event_set[\"event\"][1]\n",
    "        event = \"\".join(event_set[\"event\"])\n",
    "        entities = {\n",
    "            event_verb: {\n",
    "                \"type\": \"VERB\", \n",
    "                \"position\": re.search(event_verb, event).span()\n",
    "            }\n",
    "        }\n",
    "        for type, ent_list in event_set[\"entity_extract\"].items():\n",
    "            ent_dict_list = []\n",
    "            for ent in ent_list:\n",
    "                ent_dict = {ent: {\"type\": type}}\n",
    "                if re.search(\"\\+|\\|\", ent):\n",
    "                    re_ent = list(ent)\n",
    "                    insert_num = 0\n",
    "                    for span in re.finditer(\"\\+|\\|\", ent):\n",
    "                        icon_idx = span.span()\n",
    "                        re_ent.insert(icon_idx[0]+insert_num, \"\\\\\")\n",
    "                        insert_num += 1\n",
    "                    re_ent = \"\".join(re_ent)\n",
    "                else:\n",
    "                    re_ent = ent\n",
    "                ent_dict[ent][\"position\"] = re.search(re_ent, event).span()\n",
    "                assert ent_dict[ent][\"position\"][0] != ent_dict[ent][\"position\"][1]\n",
    "                entities.update(ent_dict)\n",
    "\n",
    "        event_set[\"entity_extract_posi\"] = entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': ['公路總局', '補助', '防疫加清消費用'],\n",
       "  'entity_extract': {'ORG': ['公路總局'], 'OTH': ['防疫']},\n",
       "  'entity_extract_posi': {'補助': {'type': 'VERB', 'position': (4, 6)},\n",
       "   '公路總局': {'type': 'ORG', 'position': (0, 4)},\n",
       "   '防疫': {'type': 'OTH', 'position': (6, 8)}}},\n",
       " {'event': ['司機', '因應', '疫情高原期影響'],\n",
       "  'entity_extract': {'OTH': ['疫情']},\n",
       "  'entity_extract_posi': {'因應': {'type': 'VERB', 'position': (2, 4)},\n",
       "   '疫情': {'type': 'OTH', 'position': (4, 6)}}},\n",
       " {'event': ['交通部',\n",
       "   '表示',\n",
       "   '行政院已同意延長從6月至年底防疫物資費用補貼並已規畫延續每日防疫補助以及新增一次性清消防護費用提高每輛計程車補貼金額共5000元'],\n",
       "  'entity_extract': {'ORG': ['行政院'],\n",
       "   'MONEY': ['5000元'],\n",
       "   'DATE': ['6月至年底'],\n",
       "   'CARDINAL': ['一次性'],\n",
       "   'OTH': ['防疫', '防疫物資費用補貼', '防疫補助', '防護費用', '計程車補貼金額'],\n",
       "   'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'表示': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '行政院': {'type': 'ORG', 'position': (5, 8)},\n",
       "   '5000元': {'type': 'MONEY', 'position': (64, 69)},\n",
       "   '6月至年底': {'type': 'DATE', 'position': (14, 19)},\n",
       "   '一次性': {'type': 'CARDINAL', 'position': (43, 46)},\n",
       "   '防疫': {'type': 'OTH', 'position': (19, 21)},\n",
       "   '防疫物資費用補貼': {'type': 'OTH', 'position': (19, 27)},\n",
       "   '防疫補助': {'type': 'OTH', 'position': (35, 39)},\n",
       "   '防護費用': {'type': 'OTH', 'position': (48, 52)},\n",
       "   '計程車補貼金額': {'type': 'OTH', 'position': (56, 63)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (56, 59)}}},\n",
       " {'event': ['行政院', '同意', '延長從6月至年底防疫物資費用補貼'],\n",
       "  'entity_extract': {'ORG': ['行政院'],\n",
       "   'DATE': ['6月至年底'],\n",
       "   'OTH': ['防疫', '防疫物資費用補貼']},\n",
       "  'entity_extract_posi': {'同意': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '行政院': {'type': 'ORG', 'position': (0, 3)},\n",
       "   '6月至年底': {'type': 'DATE', 'position': (8, 13)},\n",
       "   '防疫': {'type': 'OTH', 'position': (13, 15)},\n",
       "   '防疫物資費用補貼': {'type': 'OTH', 'position': (13, 21)}}},\n",
       " {'event': ['交通部', '請', '公路總局'],\n",
       "  'entity_extract': {'ORG': ['公路總局']},\n",
       "  'entity_extract_posi': {'請': {'type': 'VERB', 'position': (3, 4)},\n",
       "   '公路總局': {'type': 'ORG', 'position': (4, 8)}}},\n",
       " {'event': ['公路總局', '完成', '準備作業'],\n",
       "  'entity_extract': {'ORG': ['公路總局'], 'OTH': ['作業']},\n",
       "  'entity_extract_posi': {'完成': {'type': 'VERB', 'position': (4, 6)},\n",
       "   '公路總局': {'type': 'ORG', 'position': (0, 4)},\n",
       "   '作業': {'type': 'OTH', 'position': (8, 10)}}},\n",
       " {'event': ['公路總局', '進行', '撥付'],\n",
       "  'entity_extract': {'ORG': ['公路總局']},\n",
       "  'entity_extract_posi': {'進行': {'type': 'VERB', 'position': (4, 6)},\n",
       "   '公路總局': {'type': 'ORG', 'position': (0, 4)}}},\n",
       " {'event': ['交通部',\n",
       "   '表示',\n",
       "   '為協助計程車營運防疫需自110年5月起每天補貼計程車防疫物資費用15元每個月補貼26天原本至今年5月底止在行政院支持下將延長至今年年底'],\n",
       "  'entity_extract': {'ORG': ['行政院'],\n",
       "   'MONEY': ['15元'],\n",
       "   'DATE': ['26天', '110年5月', '今年年底', '今年5月底止'],\n",
       "   'OTH': ['防疫', '計程車營運防疫', '補貼計程車防疫物資費用'],\n",
       "   'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'表示': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '行政院': {'type': 'ORG', 'position': (58, 61)},\n",
       "   '15元': {'type': 'MONEY', 'position': (37, 40)},\n",
       "   '26天': {'type': 'DATE', 'position': (45, 48)},\n",
       "   '110年5月': {'type': 'DATE', 'position': (17, 23)},\n",
       "   '今年年底': {'type': 'DATE', 'position': (68, 72)},\n",
       "   '今年5月底止': {'type': 'DATE', 'position': (51, 57)},\n",
       "   '防疫': {'type': 'OTH', 'position': (13, 15)},\n",
       "   '計程車營運防疫': {'type': 'OTH', 'position': (8, 15)},\n",
       "   '補貼計程車防疫物資費用': {'type': 'OTH', 'position': (26, 37)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (8, 11)}}},\n",
       " {'event': ['計程車', '載運', '不特定乘客'],\n",
       "  'entity_extract': {'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'載運': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (0, 3)}}},\n",
       " {'event': ['由交通部防疫預算', '支出', '經費'],\n",
       "  'entity_extract': {'ORG': ['交通部'], 'OTH': ['防疫']},\n",
       "  'entity_extract_posi': {'支出': {'type': 'VERB', 'position': (8, 10)},\n",
       "   '交通部': {'type': 'ORG', 'position': (1, 4)},\n",
       "   '防疫': {'type': 'OTH', 'position': (4, 6)}}},\n",
       " {'event': ['交通部', '規畫', '階段性汽燃費減徵支持振興措施'],\n",
       "  'entity_extract': {'ORG': ['交通部']},\n",
       "  'entity_extract_posi': {'規畫': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '交通部': {'type': 'ORG', 'position': (0, 3)}}},\n",
       " {'event': ['者', '達', '7成'],\n",
       "  'entity_extract': {'PERCENT': ['7成']},\n",
       "  'entity_extract_posi': {'達': {'type': 'VERB', 'position': (1, 2)},\n",
       "   '7成': {'type': 'PERCENT', 'position': (2, 4)}}},\n",
       " {'event': ['每輛車', '節省', '約4000元至1萬元'],\n",
       "  'entity_extract': {'MONEY': ['4000元']},\n",
       "  'entity_extract_posi': {'節省': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '4000元': {'type': 'MONEY', 'position': (6, 11)}}},\n",
       " {'event': ['長租車', '有', '固定承租客户'],\n",
       "  'entity_extract': {},\n",
       "  'entity_extract_posi': {'有': {'type': 'VERB', 'position': (3, 4)}}},\n",
       " {'event': ['小客車租賃業部分', '規畫', '針對短租車提供111年第3季汽燃費減半徵收方案'],\n",
       "  'entity_extract': {'DATE': ['111年第3季', '第3季', '11年']},\n",
       "  'entity_extract_posi': {'規畫': {'type': 'VERB', 'position': (8, 10)},\n",
       "   '111年第3季': {'type': 'DATE', 'position': (17, 24)},\n",
       "   '第3季': {'type': 'DATE', 'position': (21, 24)},\n",
       "   '11年': {'type': 'DATE', 'position': (18, 21)}}},\n",
       " {'event': ['每輛車', '節省', '約1200元至1800元'],\n",
       "  'entity_extract': {'MONEY': ['1200元', '1200元至1800元']},\n",
       "  'entity_extract_posi': {'節省': {'type': 'VERB', 'position': (3, 5)},\n",
       "   '1200元': {'type': 'MONEY', 'position': (6, 11)},\n",
       "   '1200元至1800元': {'type': 'MONEY', 'position': (6, 17)}}},\n",
       " {'event': ['交通部',\n",
       "   '說',\n",
       "   '因應國內疫情及依據行政院現階段以防疫及振興為主政策針對目前仍受影響觀光公路客運計程車遊覽車及小客車租賃產業在行政院支持下尋求各種可運用資源已分別因應各產業規畫不同方案措施協助業者因應疫情影響並將持續與各產業同步做好振興準備'],\n",
       "  'entity_extract': {'ORG': ['行政院'], 'OTH': ['防疫', '疫情'], 'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'說': {'type': 'VERB', 'position': (3, 4)},\n",
       "   '行政院': {'type': 'ORG', 'position': (13, 16)},\n",
       "   '防疫': {'type': 'OTH', 'position': (20, 22)},\n",
       "   '疫情': {'type': 'OTH', 'position': (8, 10)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (43, 46)}}},\n",
       " {'event': ['產業', '為主', '以防疫及振興'],\n",
       "  'entity_extract': {'OTH': ['防疫']},\n",
       "  'entity_extract_posi': {'為主': {'type': 'VERB', 'position': (2, 4)},\n",
       "   '防疫': {'type': 'OTH', 'position': (5, 7)}}},\n",
       " {'event': ['產業', '受', '影響'],\n",
       "  'entity_extract': {},\n",
       "  'entity_extract_posi': {'受': {'type': 'VERB', 'position': (2, 3)}}},\n",
       " {'event': ['業者', '因應', '疫情影響'],\n",
       "  'entity_extract': {'OTH': ['疫情']},\n",
       "  'entity_extract_posi': {'因應': {'type': 'VERB', 'position': (2, 4)},\n",
       "   '疫情': {'type': 'OTH', 'position': (4, 6)}}},\n",
       " {'event': ['交通部', '擬', '發給計程車業每人一次性5000元防疫物資補貼'],\n",
       "  'entity_extract': {'ORG': ['交通部'],\n",
       "   'MONEY': ['5000元'],\n",
       "   'CARDINAL': ['一次性'],\n",
       "   'OTH': ['防疫'],\n",
       "   'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'擬': {'type': 'VERB', 'position': (3, 4)},\n",
       "   '交通部': {'type': 'ORG', 'position': (0, 3)},\n",
       "   '5000元': {'type': 'MONEY', 'position': (15, 20)},\n",
       "   '一次性': {'type': 'CARDINAL', 'position': (12, 15)},\n",
       "   '防疫': {'type': 'OTH', 'position': (20, 22)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (6, 9)}}},\n",
       " {'event': ['應疫情高原期影響，交通部表示，行政院已同意延長從6月至年底的防疫物資費用補貼，並已規畫延續每日防疫補助以及新增一次性清消防護費用，提高每輛計程車補貼金額共5000元，交通部也已請公路總局盡速完成準備作業',\n",
       "   '因',\n",
       "   '於7月進行撥付。'],\n",
       "  'entity_extract': {'MONEY': ['5000元'],\n",
       "   'ORG': ['公路總局', '行政院', '交通部'],\n",
       "   'DATE': ['7月'],\n",
       "   'CARDINAL': ['一次性'],\n",
       "   'OTH': ['防疫', '疫情', '防疫物資費用補貼', '防疫補助', '防護費用', '計程車補貼金額', '作業'],\n",
       "   'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'因': {'type': 'VERB', 'position': (101, 102)},\n",
       "   '5000元': {'type': 'MONEY', 'position': (77, 82)},\n",
       "   '公路總局': {'type': 'ORG', 'position': (89, 93)},\n",
       "   '行政院': {'type': 'ORG', 'position': (15, 18)},\n",
       "   '交通部': {'type': 'ORG', 'position': (9, 12)},\n",
       "   '7月': {'type': 'DATE', 'position': (103, 105)},\n",
       "   '一次性': {'type': 'CARDINAL', 'position': (55, 58)},\n",
       "   '防疫': {'type': 'OTH', 'position': (30, 32)},\n",
       "   '疫情': {'type': 'OTH', 'position': (1, 3)},\n",
       "   '防疫物資費用補貼': {'type': 'OTH', 'position': (30, 38)},\n",
       "   '防疫補助': {'type': 'OTH', 'position': (47, 51)},\n",
       "   '防護費用': {'type': 'OTH', 'position': (60, 64)},\n",
       "   '計程車補貼金額': {'type': 'OTH', 'position': (69, 76)},\n",
       "   '作業': {'type': 'OTH', 'position': (99, 101)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (69, 72)}}},\n",
       " {'event': ['考量計程車每日均載運不特定乘客且空間小與乘客近',\n",
       "   '因此',\n",
       "   '除延續每日防疫物資補貼6至12月底的2730元，再新增一次性因應現況疫情加強清消及防護費用2270元，每輛計程車共計5000元，將於7月以一次性撥款方式匯到計程車車主帳戶，經費由交通部防疫預算支出。'],\n",
       "  'entity_extract': {'PRODUCT': ['計程車'],\n",
       "   'MONEY': ['2270元', '5000元', '2730元'],\n",
       "   'ORG': ['交通部'],\n",
       "   'DATE': ['7月', '6至12月底'],\n",
       "   'TIME': ['6至12月底'],\n",
       "   'CARDINAL': ['一次性'],\n",
       "   'OTH': ['防疫', '疫情', '防護費用']},\n",
       "  'entity_extract_posi': {'因此': {'type': 'VERB', 'position': (23, 25)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (2, 5)},\n",
       "   '2270元': {'type': 'MONEY', 'position': (70, 75)},\n",
       "   '5000元': {'type': 'MONEY', 'position': (83, 88)},\n",
       "   '2730元': {'type': 'MONEY', 'position': (43, 48)},\n",
       "   '交通部': {'type': 'ORG', 'position': (114, 117)},\n",
       "   '7月': {'type': 'DATE', 'position': (91, 93)},\n",
       "   '6至12月底': {'type': 'TIME', 'position': (36, 42)},\n",
       "   '一次性': {'type': 'CARDINAL', 'position': (52, 55)},\n",
       "   '防疫': {'type': 'OTH', 'position': (30, 32)},\n",
       "   '疫情': {'type': 'OTH', 'position': (59, 61)},\n",
       "   '防護費用': {'type': 'OTH', 'position': (66, 70)}}},\n",
       " {'event': ['小客車租賃業部分，因長租車均有固定承租客户',\n",
       "   '故',\n",
       "   '規畫針對短租車提供111年第3季汽燃費減半徵收方案，平均每輛車可節省約1200元至1800元。'],\n",
       "  'entity_extract': {'MONEY': ['1200元', '1200元至1800元'],\n",
       "   'DATE': ['111年第3季', '第3季', '11年']},\n",
       "  'entity_extract_posi': {'故': {'type': 'VERB', 'position': (21, 22)},\n",
       "   '1200元': {'type': 'MONEY', 'position': (57, 62)},\n",
       "   '1200元至1800元': {'type': 'MONEY', 'position': (57, 68)},\n",
       "   '111年第3季': {'type': 'DATE', 'position': (31, 38)},\n",
       "   '第3季': {'type': 'DATE', 'position': (35, 38)},\n",
       "   '11年': {'type': 'DATE', 'position': (32, 35)}}},\n",
       " {'event': ['應國內疫情及依據行政院現階段以防疫及振興為主的政策，針對目前仍受影響的觀光、公路客運、計程車、遊覽車及小客車租賃等產業，在行政院支持下，尋求各種可運用的資源，已分別因應各產業規畫不同的方案措施，協助業者因應疫情的影響',\n",
       "   '因',\n",
       "   '並將持續與各產業同步做好的振興準備。'],\n",
       "  'entity_extract': {'ORG': ['行政院'], 'OTH': ['防疫', '疫情'], 'PRODUCT': ['計程車']},\n",
       "  'entity_extract_posi': {'因': {'type': 'VERB', 'position': (82, 83)},\n",
       "   '行政院': {'type': 'ORG', 'position': (8, 11)},\n",
       "   '防疫': {'type': 'OTH', 'position': (15, 17)},\n",
       "   '疫情': {'type': 'OTH', 'position': (3, 5)},\n",
       "   '計程車': {'type': 'PRODUCT', 'position': (43, 46)}}}]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"event_triplets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_event_sents = []\n",
    "for article in data:\n",
    "    for event_item in article[\"event_triplets\"]:\n",
    "        all_event_sents.append({\n",
    "            \"event_text\": \"\".join(event_item[\"event\"]),\n",
    "            \"event_positions\": event_item[\"entity_extract_posi\"]\n",
    "        })\n",
    "\n",
    "max_length = max([len(sent[\"event_text\"]) for sent in all_event_sents])\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataset(torch.utils.data.Dataset):\n",
    "    # 讀取資料、參數初始化\n",
    "    def __init__(self, sentences, tokenizer, max_length):\n",
    "        self.sentences = sentences\n",
    "        self.len = len(self.sentences)\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    # 回傳一筆訓練/測試資料\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.sentences[idx][\"event_text\"]\n",
    "        entities = self.sentences[idx][\"event_positions\"]\n",
    "        tokens = self.tokenizer(text, max_length=self.max_length, padding=\"max_length\", truncation=True)\n",
    "        tokens_tensor = torch.tensor(tokens[\"input_ids\"])\n",
    "        segments_tensors = torch.tensor(tokens[\"token_type_ids\"])\n",
    "        masks_tensors = torch.tensor(tokens[\"attention_mask\"])\n",
    "        return (tokens_tensor, segments_tensors, masks_tensors, entities)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "def transform(encoded_layers):\n",
    "    token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token, dim=0)\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentenceDataset(sentences=all_event_sents, tokenizer=tokenizer, max_length=max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8772/8772 [43:10<00:00,  3.39it/s]\n"
     ]
    }
   ],
   "source": [
    "all_entities = {}\n",
    "for _data in tqdm(dataloader):\n",
    "    if next(model.parameters()).is_cuda:\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        _data, entities = [t.to(\"cuda\") for t in _data[:3] if t is not None], _data[3]\n",
    "        tokens_tensor, segments_tensors, masks_tensors = _data\n",
    "        _output = model(\n",
    "            input_ids=tokens_tensor,\n",
    "            token_type_ids=segments_tensors,\n",
    "            attention_mask=masks_tensors\n",
    "        )\n",
    "        encoded_layers = _output[2]  # sentence vector\n",
    "        # formatting the vector\n",
    "        token_vec_sum = transform(encoded_layers=encoded_layers)\n",
    "        # mapping the word vector by token vecor stack then sum up \n",
    "        for ent, values in entities.items():\n",
    "            values[\"type\"] = values[\"type\"][0]\n",
    "            # get entity position in the sentence\n",
    "            start, end = values[\"position\"][0].numpy()[0], values[\"position\"][1].numpy()[0]\n",
    "            values[\"position\"] = (start, end)\n",
    "            # compute word vector and copy to local cpu environment, to prevent CUDA OOM\n",
    "            wv = torch.sum(torch.stack(token_vec_sum[start:end]), dim=0).cpu().detach().numpy()\n",
    "\n",
    "            # if entity repeated, do torch.mean then add to all_entities(dict)\n",
    "            if ent in all_entities:\n",
    "                wv = np.mean([all_entities[ent][\"wv\"], wv], axis=0)\n",
    "            all_entities[ent] = {\"type\": values[\"type\"], \"wv\": wv}\n",
    "\n",
    "        del entities\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity embeddings 分群\n",
    "- 沒有分 type\n",
    "  - 幾乎自成一群 很分散\n",
    "- 區分不同 type 跑分群\n",
    "  - 能夠找到相似詞了，但遇到比較通用的詞彙（例如工會、市議會），及剛好在這個範圍中沒有多種講法的詞彙（例如：2353、台積電、台達）會在可以找到幾組相似詞匯的 threshold 下被集中到一大群\n",
    "- 不管誰和誰一群，只找空間中最相似者\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OTH',\n",
       " 'NORP',\n",
       " 'LAW',\n",
       " 'WORK_OF_ART',\n",
       " 'TIME',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'LOC',\n",
       " 'CARDINAL',\n",
       " 'VERB',\n",
       " 'PRODUCT',\n",
       " 'ART',\n",
       " 'PERSON',\n",
       " 'PERCENT',\n",
       " 'FAC',\n",
       " 'MONEY',\n",
       " 'QUANTITY',\n",
       " 'EVENT',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'DATE']"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5230/5230 [00:00<00:00, 29675.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(690, 1024)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types = list(set([values[\"type\"] for ent, values in all_entities.items()]))\n",
    "all_entity_vectors = {}\n",
    "for ent, values in tqdm(all_entities.items()):\n",
    "    if values[\"type\"] in all_entity_vectors:\n",
    "        all_entity_vectors[values[\"type\"]].append(values[\"wv\"].tolist())\n",
    "        all_entity_vectors[values[\"type\"]+\"-name\"].append(ent)\n",
    "    else:\n",
    "        all_entity_vectors[values[\"type\"]] = [values[\"wv\"].tolist()]\n",
    "        all_entity_vectors[values[\"type\"]+\"-name\"] = [ent]\n",
    "\n",
    "for type in all_entity_vectors:\n",
    "    all_entity_vectors[type] = np.array(all_entity_vectors[type])\n",
    "\n",
    "print(types)\n",
    "all_entity_vectors[\"ORG\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import manifold\n",
    "\n",
    "from scipy.cluster.hierarchy import single, ward, fcluster\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn\n",
    "clusterer = AgglomerativeClustering(n_clusters=None, affinity=\"manhattan\", linkage=\"single\", distance_threshold=3)\n",
    "clusterer.fit(all_entity_vectors)\n",
    "Counter(clusterer.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kmeans\n",
    "clusterer = KMeans(...)\n",
    "clusterer.fit(all_entity_vectors)\n",
    "Counter(clusterer.labels_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scipy\n",
    "_vectors = all_entity_vectors[\"EVENT\"]\n",
    "_ents = all_entity_vectors[\"EVENT-name\"]\n",
    "\n",
    "distance_Y = pdist(_vectors, 'euclidean')\n",
    "linkage_Z = single(distance_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "南北戰爭 北戰爭 480.80251561789987\n",
      "美國南北戰爭 南北戰爭 1020.3843476979752\n",
      "對話會 座談會 969.5389822597508\n",
      "九二 二戰 812.4386289167896\n",
      "事件 會議 781.7473317594171\n",
      "唐山黑社會團夥 黑道團伙事件 1348.100737310031\n",
      "俄烏戰爭 烏俄戰爭 721.2171839820406\n",
      "會議 集會 551.9291054845628\n",
      "集會 會議 551.9291054845628\n",
      "聯誼會 集會 861.0308424045578\n",
      "台日聯誼會 聯誼會 1019.4234739834417\n",
      "燒烤店群毆女子案 離譜毆女事件 1903.1026226708648\n",
      "烏克蘭戰爭 俄烏戰爭 1024.779129600513\n",
      "部長會議 集會 902.4373769175987\n",
      "放映會 集會 1013.3116176163201\n",
      "城市博覽會 畢業典禮 1174.5196122027326\n",
      "專項行動 部長會議 1069.3176372291684\n",
      "雷霆風暴 集會 1272.3544401926586\n",
      "專項行動動員部署大會 專項行動 2352.9247829799815\n",
      "歐洲國際人力潛艇競賽 國際勞工大會 2071.579895366774\n",
      "票選活動 畢業典禮 984.6216136250993\n",
      "理監事會 理監事會議 643.413230367707\n",
      "第2季理監事會 理監事會議 1151.6225575052097\n",
      "選舉 集會 699.6396029675989\n",
      "安全會議 部長會議 1126.8177383297716\n",
      "北戰爭 南北戰爭 480.80251561789987\n",
      "行動集會 集會 1118.0198257434502\n",
      "遊行集會 集會 1079.8727186764916\n",
      "日經 會議 619.4098983583347\n",
      "電影交響音樂 電影交響音樂會 509.1367063523519\n",
      "電影交響音樂會 電影交響音樂 509.1367063523519\n",
      "記者會 集會 902.6343532342445\n",
      "畢業典禮 會議 914.0039439396606\n",
      "校區畢業典禮 畢業典禮 1289.0637787217436\n",
      "亞洲安全會議 安全會議 1262.6512490909056\n",
      "座談會 集會 868.0226740413076\n",
      "防疫會 記者會 911.4691294326634\n",
      "暴力事件 事件 1088.6101731282288\n",
      "黑道團伙事件 唐山黑社會團夥 1348.100737310031\n",
      "離譜毆女事件 暴力事件 1476.3046310800262\n",
      "六四事件 二戰 1038.3252723965604\n",
      "恩恩案 集會 1031.8684831297303\n",
      "防疫記者會 記者會 959.4312850853013\n",
      "全代會 集會 872.7687324165754\n",
      "香格里拉對話會議 對話會議 1503.127521082246\n",
      "對話會議 俄烏戰爭 1107.2554246009072\n",
      "香格里拉安全峰會 亞洲安全會議 1713.4201655789022\n",
      "演練 集會 644.2430390958933\n",
      "烏俄戰爭 俄烏戰爭 721.2171839820406\n",
      "哥本哈根民主高峰會 德州槍擊案 3025.124170506461\n",
      "福島核子事故 烏俄戰爭 1531.8485333286862\n",
      "理監事會議 理監事會 643.413230367707\n",
      "德州槍擊案 票選活動 1103.7410876752072\n",
      "國際勞工大會 部長會議 1406.9265283270145\n",
      "麵包大賽 集會 1200.62941601876\n",
      "路易.樂斯福世界盃 香格里拉安全峰會 2053.0593762247563\n",
      "健力賽 集會 974.8871455080808\n",
      "中油高煉廠改建大建案 德州槍擊案 2371.052259701112\n",
      "二戰 日經 685.2727977454406\n"
     ]
    }
   ],
   "source": [
    "# find most simliar \n",
    "distance_matrix_Y = squareform(distance_Y)\n",
    "\n",
    "for query_eid in range(distance_matrix_Y.shape[0]):\n",
    "    _pool = distance_matrix_Y[query_eid, :]\n",
    "    nearest_eid = np.argpartition(_pool, 2)[1]\n",
    "    print(_ents[query_eid], _ents[nearest_eid], _pool[nearest_eid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust dendrogram threshold\n",
    "cluster_label = fcluster(linkage_Z, 500, criterion=\"distance\")\n",
    "cluster_dict = dict(Counter(cluster_label))\n",
    "cluster_dict = {k: v for k, v in sorted(cluster_dict.items(), key=lambda item: item[1], reverse=True)}\n",
    "cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "265it [00:00, 592478.98it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_agglomer = {}\n",
    "for entity_id, cluster_id in tqdm(enumerate(cluster_label)):\n",
    "    if cluster_id not in entity_agglomer:\n",
    "        entity_agglomer[cluster_id] = []\n",
    "\n",
    "    entity_agglomer[cluster_id].append(_ents[entity_id])\n",
    "\n",
    "entity_agglomer = {k: v for k, v in sorted(entity_agglomer.items(), key=lambda item: len(item[1]), reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  78   22\n",
      "['中方', '道瓊', '中央', '企業', '央行', '中影', '美軍', '美中', '中共', '共軍', '共機', '共艦', '工會', '公司', '中油', '華信', '央視', '美銀', '車廠', '中企', '路透', '國安']\n",
      "\n",
      "Cluster  65   2\n",
      "['行政院', '政院']\n",
      "\n",
      "Cluster  77   2\n",
      "['俄軍', '俄砲']\n",
      "\n",
      "Cluster  74   2\n",
      "['歐盟', '白紙']\n",
      "\n",
      "Cluster  72   2\n",
      "['立院', '立委']\n",
      "\n",
      "Cluster  142   1\n",
      "['交通部']\n",
      "\n",
      "Cluster  188   1\n",
      "['道奇隊']\n",
      "\n",
      "Cluster  152   1\n",
      "['陸戰隊']\n",
      "\n",
      "Cluster  62   1\n",
      "['民進黨']\n",
      "\n",
      "Cluster  54   1\n",
      "['中國軍隊']\n",
      "\n",
      "Cluster  147   1\n",
      "['國防部長']\n",
      "\n",
      "Cluster  1   1\n",
      "['大陸衛健委']\n",
      "\n",
      "Cluster  2   1\n",
      "['衛健委']\n",
      "\n",
      "Cluster  209   1\n",
      "['中國代表團']\n",
      "\n",
      "Cluster  116   1\n",
      "['外交']\n",
      "\n",
      "Cluster  11   1\n",
      "['中央氣象局']\n",
      "\n",
      "Cluster  130   1\n",
      "['亞馬遜']\n",
      "\n",
      "Cluster  223   1\n",
      "['環球報']\n",
      "\n",
      "Cluster  224   1\n",
      "['Azot']\n",
      "\n",
      "Cluster  90   1\n",
      "['路透社']\n",
      "\n",
      "Cluster  86   1\n",
      "['媒體']\n",
      "\n",
      "Cluster  33   1\n",
      "['外交部']\n",
      "\n",
      "Cluster  129   1\n",
      "['台積電']\n",
      "\n",
      "Cluster  175   1\n",
      "['內政部']\n",
      "\n",
      "Cluster  124   1\n",
      "['當局']\n",
      "\n",
      "Cluster  160   1\n",
      "['Fed']\n",
      "\n",
      "Cluster  68   1\n",
      "['聯準會']\n",
      "\n",
      "Cluster  104   1\n",
      "['政府']\n",
      "\n",
      "Cluster  69   1\n",
      "['聯準']\n",
      "\n",
      "Cluster  229   1\n",
      "['Beta']\n",
      "\n",
      "Cluster  213   1\n",
      "['高貝他']\n",
      "\n",
      "Cluster  190   1\n",
      "['美國聯準會']\n",
      "\n",
      "Cluster  165   1\n",
      "['WTO']\n",
      "\n",
      "Cluster  167   1\n",
      "['勞動部']\n",
      "\n",
      "Cluster  95   1\n",
      "['工廠']\n",
      "\n",
      "Cluster  89   1\n",
      "['分會']\n",
      "\n",
      "Cluster  17   1\n",
      "['市議會']\n",
      "\n",
      "Cluster  18   1\n",
      "['澀川市議會']\n",
      "\n",
      "Cluster  102   1\n",
      "['警方']\n",
      "\n",
      "Cluster  133   1\n",
      "['陸官']\n",
      "\n",
      "Cluster  110   1\n",
      "['美媒']\n",
      "\n",
      "Cluster  186   1\n",
      "['麥當勞']\n",
      "\n",
      "Cluster  126   1\n",
      "['雙方']\n",
      "\n",
      "Cluster  156   1\n",
      "['中防長']\n",
      "\n",
      "Cluster  22   1\n",
      "['英國國防部']\n",
      "\n",
      "Cluster  117   1\n",
      "['國防部']\n",
      "\n",
      "Cluster  161   1\n",
      "['俄羅斯軍隊']\n",
      "\n",
      "Cluster  237   1\n",
      "['蘇寧易購']\n",
      "\n",
      "Cluster  82   1\n",
      "['國會']\n",
      "\n",
      "Cluster  136   1\n",
      "['民主黨']\n",
      "\n",
      "Cluster  58   1\n",
      "['法院']\n",
      "\n",
      "Cluster  140   1\n",
      "['法新社']\n",
      "\n",
      "Cluster  231   1\n",
      "['俄羅斯國防部']\n",
      "\n",
      "Cluster  194   1\n",
      "['聯合國']\n",
      "\n",
      "Cluster  235   1\n",
      "['市政府']\n",
      "\n",
      "Cluster  178   1\n",
      "['中央社']\n",
      "\n",
      "Cluster  56   1\n",
      "['俄國軍隊']\n",
      "\n",
      "Cluster  40   1\n",
      "['大陸國務院']\n",
      "\n",
      "Cluster  41   1\n",
      "['國務院']\n",
      "\n",
      "Cluster  238   1\n",
      "['富邦人壽']\n",
      "\n",
      "Cluster  196   1\n",
      "['台灣人壽']\n",
      "\n",
      "Cluster  184   1\n",
      "['兆豐金']\n",
      "\n",
      "Cluster  120   1\n",
      "['中信金']\n",
      "\n",
      "Cluster  75   1\n",
      "['台壽']\n",
      "\n",
      "Cluster  150   1\n",
      "['大金']\n",
      "\n",
      "Cluster  233   1\n",
      "['花旗銀']\n",
      "\n",
      "Cluster  48   1\n",
      "['GIC']\n",
      "\n",
      "Cluster  139   1\n",
      "['大學']\n",
      "\n",
      "Cluster  128   1\n",
      "['團隊']\n",
      "\n",
      "Cluster  132   1\n",
      "['北約']\n",
      "\n",
      "Cluster  192   1\n",
      "['美國財政部']\n",
      "\n",
      "Cluster  60   1\n",
      "['財政部']\n",
      "\n",
      "Cluster  29   1\n",
      "['台灣央行']\n",
      "\n",
      "Cluster  201   1\n",
      "['中央銀行']\n",
      "\n",
      "Cluster  240   1\n",
      "['韓國央行']\n",
      "\n",
      "Cluster  23   1\n",
      "['新國會']\n",
      "\n",
      "Cluster  96   1\n",
      "['聯盟']\n",
      "\n",
      "Cluster  3   1\n",
      "['社會聯盟']\n",
      "\n",
      "Cluster  4   1\n",
      "['NUPES']\n",
      "\n",
      "Cluster  202   1\n",
      "['綠黨']\n",
      "\n",
      "Cluster  66   1\n",
      "['軍隊']\n",
      "\n",
      "Cluster  219   1\n",
      "['美國國防部']\n",
      "\n",
      "Cluster  19   1\n",
      "['中國國防部長']\n",
      "\n",
      "Cluster  20   1\n",
      "['美國國防部長']\n",
      "\n",
      "Cluster  46   1\n",
      "['陸委會']\n",
      "\n",
      "Cluster  127   1\n",
      "['電商']\n",
      "\n",
      "Cluster  25   1\n",
      "['度電商公']\n",
      "\n",
      "Cluster  26   1\n",
      "['印度電商公司']\n",
      "\n",
      "Cluster  80   1\n",
      "['微軟']\n",
      "\n",
      "Cluster  49   1\n",
      "['沃爾瑪']\n",
      "\n",
      "Cluster  115   1\n",
      "['軟銀']\n",
      "\n",
      "Cluster  97   1\n",
      "['世貿']\n",
      "\n",
      "Cluster  176   1\n",
      "['教育部']\n",
      "\n",
      "Cluster  50   1\n",
      "['保德信']\n",
      "\n",
      "Cluster  111   1\n",
      "['法人']\n",
      "\n",
      "Cluster  52   1\n",
      "['婦聯會']\n",
      "\n",
      "Cluster  53   1\n",
      "['基金會']\n",
      "\n",
      "Cluster  211   1\n",
      "['台北分署']\n",
      "\n",
      "Cluster  64   1\n",
      "['國民黨']\n",
      "\n",
      "Cluster  109   1\n",
      "['國產署']\n",
      "\n",
      "Cluster  103   1\n",
      "['明報']\n",
      "\n",
      "Cluster  174   1\n",
      "['中國電信']\n",
      "\n",
      "Cluster  47   1\n",
      "['農委會']\n",
      "\n",
      "Cluster  13   1\n",
      "['全宇生技']\n",
      "\n",
      "Cluster  146   1\n",
      "['品牌']\n",
      "\n",
      "Cluster  35   1\n",
      "['觀光局']\n",
      "\n",
      "Cluster  12   1\n",
      "['氣象局']\n",
      "\n",
      "Cluster  207   1\n",
      "['投信法人']\n",
      "\n",
      "Cluster  92   1\n",
      "['復華']\n",
      "\n",
      "Cluster  79   1\n",
      "['非工會']\n",
      "\n",
      "Cluster  105   1\n",
      "['單位']\n",
      "\n",
      "Cluster  168   1\n",
      "['自衛隊']\n",
      "\n",
      "Cluster  239   1\n",
      "['防衛大臣']\n",
      "\n",
      "Cluster  84   1\n",
      "['海軍']\n",
      "\n",
      "Cluster  197   1\n",
      "['行政院長']\n",
      "\n",
      "Cluster  138   1\n",
      "['爵士']\n",
      "\n",
      "Cluster  227   1\n",
      "['爵士樂隊']\n",
      "\n",
      "Cluster  38   1\n",
      "['陽明交通大學']\n",
      "\n",
      "Cluster  83   1\n",
      "['鴻海']\n",
      "\n",
      "Cluster  81   1\n",
      "['華碩']\n",
      "\n",
      "Cluster  24   1\n",
      "['2353']\n",
      "\n",
      "Cluster  148   1\n",
      "['IBM']\n",
      "\n",
      "Cluster  39   1\n",
      "['國立陽明交通大學']\n",
      "\n",
      "Cluster  155   1\n",
      "['惠普']\n",
      "\n",
      "Cluster  134   1\n",
      "['交大']\n",
      "\n",
      "Cluster  179   1\n",
      "['HP']\n",
      "\n",
      "Cluster  199   1\n",
      "['百老匯']\n",
      "\n",
      "Cluster  123   1\n",
      "['警局']\n",
      "\n",
      "Cluster  222   1\n",
      "['華府會']\n",
      "\n",
      "Cluster  162   1\n",
      "['南韓政府']\n",
      "\n",
      "Cluster  31   1\n",
      "['台灣中油']\n",
      "\n",
      "Cluster  15   1\n",
      "['台灣中油公司']\n",
      "\n",
      "Cluster  16   1\n",
      "['中油公司']\n",
      "\n",
      "Cluster  203   1\n",
      "['經創部']\n",
      "\n",
      "Cluster  166   1\n",
      "['經濟部']\n",
      "\n",
      "Cluster  189   1\n",
      "['工研院']\n",
      "\n",
      "Cluster  61   1\n",
      "['財政部長']\n",
      "\n",
      "Cluster  7   1\n",
      "['三星電子']\n",
      "\n",
      "Cluster  8   1\n",
      "['三星集團']\n",
      "\n",
      "Cluster  236   1\n",
      "['西門子']\n",
      "\n",
      "Cluster  183   1\n",
      "['ARM']\n",
      "\n",
      "Cluster  137   1\n",
      "['安謀']\n",
      "\n",
      "Cluster  170   1\n",
      "['英特爾']\n",
      "\n",
      "Cluster  34   1\n",
      "['外交部長']\n",
      "\n",
      "Cluster  149   1\n",
      "['調閱小組']\n",
      "\n",
      "Cluster  113   1\n",
      "['小組']\n",
      "\n",
      "Cluster  73   1\n",
      "['立法院']\n",
      "\n",
      "Cluster  173   1\n",
      "['國民黨團']\n",
      "\n",
      "Cluster  157   1\n",
      "['中評委']\n",
      "\n",
      "Cluster  42   1\n",
      "['中常委']\n",
      "\n",
      "Cluster  43   1\n",
      "['中常會']\n",
      "\n",
      "Cluster  180   1\n",
      "['蘇系']\n",
      "\n",
      "Cluster  153   1\n",
      "['中執會']\n",
      "\n",
      "Cluster  101   1\n",
      "['賴系']\n",
      "\n",
      "Cluster  187   1\n",
      "['總統府']\n",
      "\n",
      "Cluster  14   1\n",
      "['星宇航空']\n",
      "\n",
      "Cluster  210   1\n",
      "['民航局']\n",
      "\n",
      "Cluster  85   1\n",
      "['華信航']\n",
      "\n",
      "Cluster  32   1\n",
      "['台灣虎航']\n",
      "\n",
      "Cluster  51   1\n",
      "['德安航']\n",
      "\n",
      "Cluster  131   1\n",
      "['立榮航']\n",
      "\n",
      "Cluster  118   1\n",
      "['二工處']\n",
      "\n",
      "Cluster  21   1\n",
      "['大陸國防部長']\n",
      "\n",
      "Cluster  151   1\n",
      "['軍地']\n",
      "\n",
      "Cluster  107   1\n",
      "['央廣網']\n",
      "\n",
      "Cluster  212   1\n",
      "['陽明交大學']\n",
      "\n",
      "Cluster  44   1\n",
      "['陽明交大']\n",
      "\n",
      "Cluster  45   1\n",
      "['國立陽明交大']\n",
      "\n",
      "Cluster  76   1\n",
      "['台塑']\n",
      "\n",
      "Cluster  230   1\n",
      "['航空自衛隊']\n",
      "\n",
      "Cluster  63   1\n",
      "['自民黨']\n",
      "\n",
      "Cluster  169   1\n",
      "['北基']\n",
      "\n",
      "Cluster  191   1\n",
      "['選對會']\n",
      "\n",
      "Cluster  214   1\n",
      "['立法院長']\n",
      "\n",
      "Cluster  177   1\n",
      "['烏入會']\n",
      "\n",
      "Cluster  145   1\n",
      "['委員會']\n",
      "\n",
      "Cluster  27   1\n",
      "['台灣民眾黨']\n",
      "\n",
      "Cluster  28   1\n",
      "['民眾黨']\n",
      "\n",
      "Cluster  30   1\n",
      "['台灣國會']\n",
      "\n",
      "Cluster  94   1\n",
      "['福島']\n",
      "\n",
      "Cluster  208   1\n",
      "['核電廠']\n",
      "\n",
      "Cluster  158   1\n",
      "['監院']\n",
      "\n",
      "Cluster  195   1\n",
      "['監察院']\n",
      "\n",
      "Cluster  228   1\n",
      "['金防部']\n",
      "\n",
      "Cluster  141   1\n",
      "['微博']\n",
      "\n",
      "Cluster  5   1\n",
      "['中國交銀']\n",
      "\n",
      "Cluster  6   1\n",
      "['中國交銀國際']\n",
      "\n",
      "Cluster  87   1\n",
      "['貿協']\n",
      "\n",
      "Cluster  67   1\n",
      "['軍方']\n",
      "\n",
      "Cluster  204   1\n",
      "['環球網']\n",
      "\n",
      "Cluster  159   1\n",
      "['新墨州']\n",
      "\n",
      "Cluster  164   1\n",
      "['台經院']\n",
      "\n",
      "Cluster  163   1\n",
      "['教部']\n",
      "\n",
      "Cluster  206   1\n",
      "['教育署']\n",
      "\n",
      "Cluster  98   1\n",
      "['官方']\n",
      "\n",
      "Cluster  232   1\n",
      "['美國最高法院']\n",
      "\n",
      "Cluster  59   1\n",
      "['最高法院']\n",
      "\n",
      "Cluster  172   1\n",
      "['蓋洛普']\n",
      "\n",
      "Cluster  91   1\n",
      "['大會']\n",
      "\n",
      "Cluster  93   1\n",
      "['房仲']\n",
      "\n",
      "Cluster  125   1\n",
      "['滴滴']\n",
      "\n",
      "Cluster  55   1\n",
      "['中國政府']\n",
      "\n",
      "Cluster  135   1\n",
      "['新東方']\n",
      "\n",
      "Cluster  112   1\n",
      "['新廠']\n",
      "\n",
      "Cluster  226   1\n",
      "['小港廠']\n",
      "\n",
      "Cluster  216   1\n",
      "['屏南新廠']\n",
      "\n",
      "Cluster  182   1\n",
      "['電芯廠']\n",
      "\n",
      "Cluster  215   1\n",
      "['蘋果公司']\n",
      "\n",
      "Cluster  144   1\n",
      "['中碳']\n",
      "\n",
      "Cluster  154   1\n",
      "['碩禾']\n",
      "\n",
      "Cluster  181   1\n",
      "['榮炭']\n",
      "\n",
      "Cluster  100   1\n",
      "['台達電']\n",
      "\n",
      "Cluster  70   1\n",
      "['蔚來']\n",
      "\n",
      "Cluster  71   1\n",
      "['理想']\n",
      "\n",
      "Cluster  143   1\n",
      "['比亞迪']\n",
      "\n",
      "Cluster  88   1\n",
      "['大眾']\n",
      "\n",
      "Cluster  234   1\n",
      "['特斯拉']\n",
      "\n",
      "Cluster  218   1\n",
      "['，特斯拉']\n",
      "\n",
      "Cluster  221   1\n",
      "['國內業者']\n",
      "\n",
      "Cluster  122   1\n",
      "['華藥']\n",
      "\n",
      "Cluster  205   1\n",
      "['航運商']\n",
      "\n",
      "Cluster  99   1\n",
      "['蘋果']\n",
      "\n",
      "Cluster  217   1\n",
      "['美國蘋果']\n",
      "\n",
      "Cluster  225   1\n",
      "['中企低報']\n",
      "\n",
      "Cluster  57   1\n",
      "['中國鉬業']\n",
      "\n",
      "Cluster  106   1\n",
      "['藥廠']\n",
      "\n",
      "Cluster  119   1\n",
      "['美國藥廠']\n",
      "\n",
      "Cluster  36   1\n",
      "['觀光廳']\n",
      "\n",
      "Cluster  37   1\n",
      "['日本觀光廳']\n",
      "\n",
      "Cluster  108   1\n",
      "['澤功']\n",
      "\n",
      "Cluster  198   1\n",
      "['監管機構']\n",
      "\n",
      "Cluster  220   1\n",
      "['ACM']\n",
      "\n",
      "Cluster  121   1\n",
      "['昆凌']\n",
      "\n",
      "Cluster  200   1\n",
      "['開發商']\n",
      "\n",
      "Cluster  185   1\n",
      "['微星']\n",
      "\n",
      "Cluster  193   1\n",
      "['北醫']\n",
      "\n",
      "Cluster  171   1\n",
      "['經發局']\n",
      "\n",
      "Cluster  9   1\n",
      "['塞爆地檢署']\n",
      "\n",
      "Cluster  10   1\n",
      "['地檢署']\n",
      "\n",
      "Cluster  114   1\n",
      "['堀江']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, cluster in entity_agglomer.items():\n",
    "    print(\"Cluster \", i, \" \", len(cluster))\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(265, 1024)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_vectors = np.array([all_entity_vectors[\"ORG\"][i].tolist() for i, cid in enumerate(cluster_label) if cid == 73])\n",
    "_ents = np.array([all_entity_vectors[\"ORG-name\"][i].tolist() for i, cid in enumerate(cluster_label) if cid == 73])\n",
    "_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAISS\n",
    "- FB 開發的最相似向量搜尋工具\n",
    "- 雖然速度很快，但不確定 `index.search` 的結果為什麼是維度 `d` 變成最近鄰居個數（不是應該取樣本最近前5嗎？） 沒確認以前先不採用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngpus = faiss.get_num_gpus()\n",
    "print(\"number of GPUs:\", ngpus)\n",
    "\n",
    "d = all_entity_vectors.shape[1]\n",
    "query = \n",
    "cpu_index = faiss.IndexFlatL2(d)\n",
    "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index)\n",
    "gpu_index.add(all_entity_vectors)  # add vectors to the index\n",
    "print(gpu_index.ntotal)\n",
    "\n",
    "k = 3  # get 3 nearest neighbors\n",
    "D, I = gpu_index.search(xq, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5230, 1024)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_entity_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('finKG_py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7f521a0181281a30bb3a8083cda0ac3478dc819c0de49e23f8f755c0f313533"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
